쿠버네티스의 사용

- 퍼블릭 클라우드 환경에서의 구성 : GKE, EKS
- 온 프레미스에서의 구성 : kubeadm

master : 전체 클러스터 관리, 배포 관리
nodes(worker) : master 로 부터 작업을 지시 받고 해당 업무를 수행하여 컨테이너를 배포

kubeadm 으로 클러스터를 구성하게 되면 조인을 위한 token 을 발행한다.

kubeadm 을 이용하여 설치한 kubernetes

최초 node 가 클러스터에 조인하기 위해서는 아래의 구성이 필요(master 에서 발행시 화면에 표기됨)
kubeadm join 211.183.3.100:6443 --token <Token 값> --discovery-token-ca-cert-hash sha256:<Hash 값>

ca-cert 는 일종의 퍼블릭 키를 의미하며 길이가 너~무 길어서 이를 해시(sha256)하여 붙여넣기 한다.

/etc/kubernetes/pki/ca.crt 파일을 아래의 shell 을 이용하여 해시할 수 있다.
openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //'

Pod(포드) : 서비스 제공의 최소단위, 1개 이상의 컨테이너로 구성된다.
일반적으로는 1개의 컨테이너로 구성하고 로그,모니터링,백업 등을 위해 사이드카 컨테이너를 사용할 수 있다. 일반 도커 컨테이너와 달리 발행 즉시 외부로의 연결이 되는 것은 아니다.

일반적으로 애플리케이션의 배포는 yaml(또는 yml) 파일을 이용하여 구성한다.

apiVersion: v1      <-- api 버전 표기
kind: Pod                <-- 포드를 만들겠다
metadata:
name: my-first-ngix     <-- 포드의 이름, 주석 등을 표기할 수 있다
spec:                                      <-- 포드의 내용물에 대한 표기(컨테이너 구성)
containers:                        <-- 포드내의 컨테이너 구성 시작

- name: my-first-nginx-ctn <--- 첫번째 컨테이너 구성
image: nginx:1.10 <--- 첫번째 컨테이너의 이미지
ports: <--- 80/tcp 를 오픈하겠다
    - containerPort: 80
    protocol: TCP
- name: test-ctn <--- 두번재 컨테이너 구성
image: centos:7
command: ["tail"] <--- 명령실행(ENTRYPOINT)
args: ["-f", "/dev/null"] <--- 명령실행(CMD)

    Pod---------192.168.104.2------------

                   [80/tcp]
        my-first-nginx-ctn                test-ctn
         <http://localhost> -> nginx 로 웹 연결 가능

포드는 도커의 컨테이너/stack 과 달리 포드 내부에 있는 컨테이너들이 포드의 자원을 공유하여 사용한다. 따라서 test-ctn 이 [http://localhost](http://localhost/) 로 접속하면 80번 포트에서 서비스 하고 있는 my-fist-nginx-ctn 으로 웹 접속이 가능하게 되는 것이다. 결국 두 컨테이너는 80번 포트를 각각 사용할 수는 없다.

Replicaset

- 고정된 수의 포드를 지속적으로 동작 시키는 것
- 포드의 상위 개념이며 포드의 설정값에 replica 부분이 추가되는 것

실습 예제)
root@master:~/0520# cat nginxrs.yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
name: my-first-ngix-rs
spec:
replicas: 3
selector:
matchLabels:
color: black                     <---- 나는 "color: black" 라벨이 있는 포드를 관리하겠다
template:
metadata:
name: my-first-ngix
labels:
color: black
spec:
containers:
- name: my-first-nginx-ctn
image: nginx:1.10
ports:
- containerPort: 80
protocol: TCP

  - name: test-ctn
    image: centos:7
    command: ["tail"]
    args: ["-f", "/dev/null"]

root@master:~/0520#

root@master:~/0520# kubectl get pod, rs
error: arguments in resource/name form must have a single resource and name
root@master:~/0520# kubectl get pod,rs
NAME                         READY   STATUS    RESTARTS   AGE
pod/my-first-ngix-rs-dz2jn   2/2     Running   0          16m
pod/my-first-ngix-rs-lcbh7   2/2     Running   0          16m
pod/my-first-ngix-rs-sptgh   2/2     Running   0          31s

NAME                               DESIRED   CURRENT   READY   AGE
replicaset.apps/my-first-ngix-rs   3         3         3       16m
root@master:~/0520# kubectl get pod -o wide
NAME                     READY   STATUS    RESTARTS   AGE     IP              NODE    NOMINATED NODE   READINESS GATES
my-first-ngix-rs-dz2jn   2/2     Running   0          18m     192.168.104.5   node2   <none>           <none>
my-first-ngix-rs-lcbh7   2/2     Running   0          18m     192.168.135.3   node3   <none>           <none>
my-first-ngix-rs-sptgh   2/2     Running   0          3m16s   192.168.104.6   node2   <none>           <none>
root@master:~/0520#

Deployment

- Pod-> ReplicaSet -> Deployment
디플로이먼트는 레플리카셋 기능에 롤링 업데이트가 추가되어있다.
서비스의 중단없이 애플리케이션을 업데이트 할 수 있다.
결국 서비스의 배포는 포드, 레플리카셋의 구성은 필요하지 않다.
롤링 업데이트 뿐만 아니라 구성의 특정 시점을 스냅샷처럼 저장할 수 있어 롤백도 가능하다.

(RS와 구성상 큰 차이점은 없다)
root@master:~/0520# cat nginxdeploy.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
name: my-first-ngix-deploy
spec:
replicas: 3
selector:
matchLabels:
color: black
template:
metadata:
name: my-first-ngix
labels:
color: black
spec:
containers:
- name: my-first-nginx-ctn
image: nginx:1.10
ports:
- containerPort: 80
protocol: TCP
root@master:~/0520#

root@master:~/0520# kubectl get pod,rs,deploy
NAME                                        READY   STATUS    RESTARTS    AGE
pod/my-first-ngix-deploy-777cfb5888-jj45j   1/1     Running   0           63s
pod/my-first-ngix-deploy-777cfb5888-qg86d   1/1     Running   0           63s
pod/my-first-ngix-deploy-777cfb5888-s5wlt   1/1     Running   0           63s

NAME                                              DESIRED   CURRENT    READY   AGE
replicaset.apps/my-first-ngix-deploy-777cfb5888   3         3          3       63s

NAME                                   READY   UP-TO-DATE   AVAILABLE    AGE
deployment.apps/my-first-ngix-deploy   3/3     3            3            63s
root@master:~/0520#
root@master:~/0520# kubectl delete pod my-first-ngix-deploy-777cfb5888-jj45j
pod "my-first-ngix-deploy-777cfb5888-jj45j" deleted

root@master:~/0520#
root@master:~/0520# kubectl get pod
NAME                                    READY   STATUS    RESTARTS   AGE
my-first-ngix-deploy-777cfb5888-6v89p   1/1     Running   0          18s <-- rs 3 유지된다
my-first-ngix-deploy-777cfb5888-qg86d   1/1     Running   0          2m31s
my-first-ngix-deploy-777cfb5888-s5wlt   1/1     Running   0          2m31s
root@master:~/0520#

실습 시나리오... 서비스배포->업데이트->롤백

root@master:~/0520# kubectl delete -f nginxdeploy.yaml
deployment.apps "my-first-ngix-deploy" deleted
root@master:~/0520#
root@master:~/0520# kubectl apply -f nginxdeploy.yaml --record
deployment.apps/my-first-ngix-deploy created
root@master:~/0520# kubectl get deploy
NAME                   READY   UP-TO-DATE   AVAILABLE   AGE
my-first-ngix-deploy   3/3     3            3           50s
root@master:~/0520#
root@master:~/0520# kubectl rollout history
error: required resource not specified
root@master:~/0520# kubectl rollout history deployment
deployment.apps/my-first-ngix-deploy
REVISION  CHANGE-CAUSE
1         kubectl apply --filename=nginxdeploy.yaml --record=true

root@master:~/0520#

root@master:~/0520# kubectl get pod
NAME                                    READY   STATUS    RESTARTS   AGE
my-first-ngix-deploy-777cfb5888-hsz86   1/1     Running   0          3m6s
my-first-ngix-deploy-777cfb5888-lq8h4   1/1     Running   0          3m6s
my-first-ngix-deploy-777cfb5888-pmjdp   1/1     Running   0          3m6s
root@master:~/0520# kubectl describe pod my-first-ngix-deploy-777cfb5888-hsz86 | grep nginx
my-first-nginx-ctn:
Image:          nginx:1.10  //nginx 1.10 통해 웹서비스 되고 있다!!!
Image ID:       docker-pullable://nginx@sha256:6202beb06ea61f44179e02ca965e8e13b961d12640101fca213efbfd145d7575
Normal  Pulled     3m22s  kubelet            Container image "nginx:1.10" already present on machine
Normal  Created    3m21s  kubelet            Created container my-first-nginx-ctn
Normal  Started    3m21s  kubelet            Started container my-first-nginx-ctn
root@master:~/0520#

디플로이먼트로 특정 내용을 바꾸어 애플리케이션을 업데이트 하는 방법을 아래와 같이 두가지 방법을 사용할 수 있다.

1. yml 파일을 열고 내용을 수정한 다음 업데이트 하는 방법
2. kubectl set image deployment [디플로이먼트이름] nginx:1.10=nginx:1.12 --record

root@master:~/0520# kubectl apply -f nginxdeploy.yaml --record
deployment.apps/my-first-ngix-deploy created

이미지를 1.10 으로 하여 배포하며  record(revision:1)

root@master:~/0520# kubectl set image deployment my-first-ngix-deploy  my-first-nginx-ctn=nginx:1.12 --record
deployment.apps/my-first-ngix-deploy image updated

컨테이너의 이미지를 1.12 로 변경하고 record(revision:2)

root@master:~/0520#
root@master:~/0520# kubectl get pod ---> 이미지 1.12 로 생성된 포드들
NAME                                    READY   STATUS        RESTARTS    AGE
my-first-ngix-deploy-6fc7b44894-6w8sd   1/1     Running       0           11s
my-first-ngix-deploy-6fc7b44894-8cdlg   1/1     Running       0           8s
my-first-ngix-deploy-6fc7b44894-vw5hw   1/1     Running       0           13s
my-first-ngix-deploy-777cfb5888-stz59   0/1     Terminating   0           72s
root@master:~/0520# kubectl describe pod my-first-ngix-deploy-6fc7b448 94-6w8sd | grep nginx:
Image:          nginx:1.12  <---- 이미지가 1.12 인것을 확인할 수 있음
Normal  Pulled     27s   kubelet            Container image "nginx:1 .12" already present on machine
root@master:~/0520#
root@master:~/0520# kubectl rollout undo deployment my-first-ngix-depl oy --to-revision=1
deployment.apps/my-first-ngix-deploy rolled back

기존의 이미지 1.10 때 포드를 만들었던 시점으로 롤백 성공!!!

root@master:~/0520#
root@master:~/0520# kubectl get pod <--- 이미지 1.10 에서 동작하는 포드들
1.10 과 1.12 에서의 포드가 다른 것을 확인할 수 있다.
NAME                                    READY   STATUS        RESTARTS    AGE
my-first-ngix-deploy-6fc7b44894-vw5hw   0/1     Terminating   0           70s
my-first-ngix-deploy-777cfb5888-crbct   1/1     Running       0           4s
my-first-ngix-deploy-777cfb5888-hjtxd   1/1     Running       0           9s
my-first-ngix-deploy-777cfb5888-rns2s   1/1     Running       0           7s
root@master:~/0520# kubectl describe pod my-first-ngix-deploy-777cfb58 88-crbct | grep nginx:1.1
Image:          nginx:1.10 <--- 1.10 으로 롤백 되었음
Normal  Pulled     29s   kubelet            Container image "nginx:1 .10" already present on machine
root@master:~/0520#

오후--> 자신만의 이미지 생성 -> docker hub 에  업로드 -> 포드 배포(노드별로) -> 외부에 서비스하기(node-port + HAProxy)

도커 파일 내용

root@master:~/0520# cat Dockerfile
FROM nginx:1.10
EXPOSE 80
ADD index.html /usr/share/nginx/html/index.html
CMD nginx -g 'daemon off;'
root@master:~/0520#

이미지 만들기  (미리 도커 허브에 이미지를 담을 공간을 만들어 두어야 합니다)

docker build -t beomtaek78/edustack:1.0 .

"beomtaek78" : 도커허브 ID
"edustack"        : 도커 허브에 올릴 이미지명
"1.0"                    : edustack 의 일종의 버전 명

만들어진 이미지는 "docker push beomtaek78/edustack:1.0" 으로 업로드(push) 가능합니다

이제, 다  되신 분들은... 앞서 만들어 두었던 nginxdeploy.yaml 파일을 이용하여 mydeploy.yaml 만들고 이를 이용하여 3개의 포드를 배포하라(version1)!!! 단, 배포시 --record 를 해 두세요

(예)
root@master:~/0520# kubectl apply -f mydeploy.yaml --record
deployment.apps/my-v1-deploy created
root@master:~/0520#
root@master:~/0520# kubectl rollout history deploy
deployment.apps/my-v1-deploy
REVISION  CHANGE-CAUSE
1         kubectl apply --filename=mydeploy.yaml --record=true

root@master:~/0520# kubectl get pod,deploy
NAME                                READY   STATUS    RESTARTS   AGE
pod/my-v1-deploy-86778f5975-f29jq   1/1     Running   0          20s
pod/my-v1-deploy-86778f5975-n6ppd   1/1     Running   0          20s
pod/my-v1-deploy-86778f5975-p9smh   1/1     Running   0          20s

NAME                           READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/my-v1-deploy   3/3     3            3           20s
root@master:~/0520#
root@master:~/0520# cat mydeploy.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
name: my-v1-deploy
spec:
replicas: 3
selector:
matchLabels:
color: black
template:
metadata:
name: my-nginx-v1
labels:
color: black
spec:
containers:
- name: my-nginx-ctn-v1
image: beomtaek78/edustack:1.0
ports:
- containerPort: 80
protocol: TCP
root@master:~/0520#

각각의 기능을 object 라 하고 그 오브젝트 내에서 service 를 사용해야만 외부 노출,외부로부터의 접속이 가능해 진다.

서비스의 배포/접근을 위해서는 service 를 사용해야 한다.

- 서비스는 "네트워크를 담당"한다
- cluster-ip : 클러스터간에만 사용되는 통신 주소 이며 외부와의 연결은 불가하다.
즉, 도커 컨테이너를 생성할 때 -p 옵션을 제외하고 컨테이너를 만드는 것과 같다.
- node-port : 포드에 접근할 수 있는 포트를 클러스터의 모든 노드에 동일하게 개방한다.
- load balance(LB) : 일반적으로 사전에 LB 가 준비되어 있는 환경에서 연결할 수 있는 방법이며 yaml 을 통해 연결이 가능해야 한다. 보통은 aws,gcp 와 같은 퍼블릭 환경에서 사용하기 용이하다.

일반적인 환경이라면 node-port  를 통해 충분히 서비스가 가능하다.

root@master:~/0520# cat servicecip.yaml
apiVersion: v1
kind: Service
metadata:
name: my-svc
spec:
ports:
- name: webport
port: 8080                 // cluster-ip 의 포트
targetPort: 80          // pod 의 포트 = 컨테이너의 포트
selector:
color: black
type: ClusterIP
root@master:~/0520#

root@master:~/0520# cat serviceNodePort.yaml
apiVersion: v1
kind: Service
metadata:
name: my-svc
spec:
ports:
- name: webport
port: 8080
targetPort: 80
selector:
color: black
type: NodePort
root@master:~/0520# kubectl get deploy,svc
NAME                           READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/my-v1-deploy   3/3     3            3           42m

NAME                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
service/kubernetes   ClusterIP   10.96.0.1        <none>        443/TCP          47h
service/my-svc       NodePort    10.107.244.106   <none>        8080:32717/TCP   75s
root@master:~/0520#

Node IP : 211.183.3.101~103 : 32717 (RDM)
                   |
                   |
                   |
Cluster IP : 10.107.244.106 : 8080
                   |
                   |
Pod(s) 192.168.X.Y : 80

고정된 NodePort 사용하기

root@master:~/0520# kubectl delete -f serviceNodePort.yaml
service "my-svc" deleted
root@master:~/0520# vi serviceNodePort.yaml
root@master:~/0520#
root@master:~/0520#
root@master:~/0520# kubectl apply -f serviceNodePort.yaml
service/my-svc created
root@master:~/0520#
root@master:~/0520# # 노드포트를 30001 로 고정하였음 nodePort 30001
root@master:~/0520#
root@master:~/0520# kubectl get svc -o wide
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE   SELECTOR
kubernetes   ClusterIP   10.96.0.1       <none>        443/TCP          47h   <none>
my-svc       NodePort    10.104.69.231   <none>        8080:30001/TCP   66s   color=black
root@master:~/0520#
root@master:~/0520# kubectl describe service my-svc
Name:                     my-svc
Namespace:                default
Labels:                   <none>
Annotations:              <none>
Selector:                 color=black
Type:                     NodePort
IP Family Policy:         SingleStack
IP Families:              IPv4
IP:                       10.104.69.231
IPs:                      10.104.69.231
Port:                     webport  8080/TCP
TargetPort:               80/TCP
NodePort:                 webport  30001/TCP
Endpoints:                192.168.104.21:80,192.168.135.12:80,192.168.166.151:80
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   <none>
root@master:~/0520#

실습 :
HAProxy 를 구성한다. 단, HAProxy 는 아래의 조건을 갖추어야 한다.

- OS : CentOS 7.0 (minimal install)
- cpu 2, RAM : 2GB , HDD : 20GB
- NIC :
ens32 : bridge -> 10.5.101/102/103/104. ____
ens33 : VMnet8(NAT) -> 211.183.3.99

웹 접속이 가능한 것을 확인 했다면 "롤업"을 하여 CentOS(Version 2) 가 보이도록 하세요

frontend jun
mode http
bind 10.5.104.96:80
default_backend min

backend min
balance     roundrobin
server  node1 211.183.3.101:30001 check
server  node2 211.183.3.102:30001 check
server  node3 211.183.3.103:30001 check

"External Traffic Policy: Cluster" : 노드에 생성된 포드가 존재하지 않더라도 클러스터 네트워크를 통해 타 노드로 연결을 유지하여 서비스를 제공해 주는 방식으로 현재 상태에서는 211.183.3.100 에 웹서비스를 위한 포드가 존재하지 않아도 해당 주소로 웹접속시 페이지가 보이게 된다. Cluster 의 경우 이러한 편리함을 제공하지만 타 노드를 경유하여 서비스가 제공되므로 트래픽 부하가 내부적으로 증가할 수 있으며 홉도 증가하게 되어 포드 입장에서는 접속자를 외부의 클라이언트가 아닌 타 노드의 IP로 오해할 수 있다.

만약 Cluster 가 아닌 "Local" 로 변경되면 "노드IP:노드Port"  로 접속했을 경우 해당 노드에 생성된 포드로만 트래픽이 전달되고 타 노드로는 연결이 되지 않게 된다. 결과적으로 211.183.3.100(master) 으로 웹 접속 시 실제로는 포드가 없으므로 웹 서비스를 제공받을 수 없게된다.

root@master:~/0520# kubectl delete svc my-svc   // 기존 서비스 제거
service "my-svc" deleted
root@master:~/0520#
root@master:~/0520# kubectl apply -f serviceETP.yaml
service/my-svc-etp created
root@master:~/0520#
root@master:~/0520# kubectl get svc
NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
kubernetes   ClusterIP   10.96.0.1      <none>        443/TCP          47h
my-svc-etp   NodePort    10.97.133.99   <none>        8080:30001/TCP   6s
root@master:~/0520#
root@master:~/0520# cat serviceETP.yaml
apiVersion: v1
kind: Service
metadata:
name: my-svc-etp
spec:
externalTrafficPolicy: Local       <--- 추가되었음
ports:
- name: webport
port: 8080
targetPort: 80
nodePort: 30001
selector:
color: black
type: NodePort
root@master:~/0520# kubectl get svc -o wide
NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE    SELECTOR
kubernetes   ClusterIP   10.96.0.1      <none>        443/TCP          47h    <none>
my-svc-etp   NodePort    10.97.133.99   <none>        8080:30001/TCP   2m6s   color=black
root@master:~/0520# kubectl describe svc my-svc-etp
Name:                     my-svc-etp
Namespace:                default
[중간생략]
External Traffic Policy:  Local
Events:                   <none>
root@master:~/0520#

[확인] node3  에서 확인하였음
root@node3:/home/docker# curl 211.183.3.100:30001 2> /dev/null | head -2

^C   (접속되지 않아 Ctrl+C 로 강제 종료함)

root@node3:/home/docker# curl 211.183.3.101:30001 2> /dev/null | head -2
<!DOCTYPE html>
<html lang="en">

root@node3:/home/docker# curl 211.183.3.102:30001 2> /dev/null | head -2
<!DOCTYPE html>
<html lang="en">
root@node3:/home/docker#

현재 3대의 노드에서 2개의 포드가 동작중에 있다.

우리는 잠시후부터 시스템 점검을 시작해야 한다.
포드를 하나 만들어 배포하고 해당 포드는 라벨(color: white)을 적용하여 배포한다.
좀전까지 로드밸런서로  접속하면 정상적인 웹페이지 접속이 가능했었다. 이제 기존 포드와 신규 포드가 동시에 동작중인 상태에서 접속해 보면 화면에는 "시스템 점검중입니다 : Under Construction" 이 표기 되어야 한다!! 이를 *라벨 셀렉터* 로 간단히 조정해야 한다.

deployment.yaml -> service.yaml

네임스페이스(Namespace) :

- 논리적으로 분리된 작업공간
- 별도의 네임스페이스에 pod,deployment,service 등을 모아서 관리할 수 있으며 자원 사용량도 네임스페이스별로 할당 할 수 있다.